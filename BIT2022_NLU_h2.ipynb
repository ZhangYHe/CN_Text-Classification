{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1.导入需要的库"
      ],
      "metadata": {
        "id": "3cBl3KRxFkoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EL9dmdGFELRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a540ae5-7e1a-4698-d273-ae1f99fdea4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvy96g4k4o7b",
        "outputId": "ed19cb4a-17a1-4428-f763-d8b39072845c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.26.44-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.4.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.44\n",
            "  Downloading botocore-1.29.44-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.44->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.44->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch_pretrained_bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.26.44 botocore-1.29.44 jmespath-1.0.1 pytorch_pretrained_bert-0.6.2 s3transfer-0.6.0 urllib3-1.26.13\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_pretrained_bert\n",
        "# coding: UTF-8\n",
        "import torch\n",
        "import time \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig, BertAdam\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "from tqdm import tqdm \n",
        "from torch.utils.data import Dataset, TensorDataset,RandomSampler,DataLoader,SequentialSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.数据处理"
      ],
      "metadata": {
        "id": "AmUQN0VKFsnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word_ids存放词语的id\n",
        "# word_types中0、1区分不同句子\n",
        "# word_masks为attention中的掩码，0表示padding\n",
        "word_ids = []\n",
        "word_types = []\n",
        "word_masks = []\n",
        "labels = []\n",
        "pad_size = 50\n",
        "# 句子类别共10种\n",
        "sentence_types = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
        "len_dict = dict()\n",
        "\n",
        "# data_path为数据集路径，bert_path为预训练模型权重路径,ckpt_path为模型路径\n",
        "data_path = \"/content/drive/MyDrive/data/cnews/\"\n",
        "bert_path = \"/content/drive/MyDrive/chinese_roberta_wwm_ext_pytorch/\"\n",
        "ckpt_path = \"/content/drive/MyDrive/data/ckpt/\"\n",
        "# 初始化分词器，使用预训练的vocab\n",
        "tokenizer = BertTokenizer(vocab_file=bert_path + \"vocab.txt\")\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 3\n",
        "optimizer_lr = 2e-5\n",
        "optimizer_warmup = 0.05"
      ],
      "metadata": {
        "id": "J2iHbuRe4y-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 读取训练数据\n",
        "with open(data_path + \"cnews.test.txt\", encoding='utf-8') as f:\n",
        "    for i, item in tqdm(enumerate(f)):\n",
        "        # 统计句子长度，存储在字典中\n",
        "        if (len(item)-4) not in len_dict:\n",
        "          len_dict[len(item)-4] = 1\n",
        "        else:\n",
        "          len_dict[len(item)-4] += 1\n",
        "        # 句子类别为字符串第一个词语\n",
        "        item_type = sentence_types.index(item[0:2])\n",
        "        sentence = tokenizer.tokenize(item[4:-1])\n",
        "        # 添加bert中句首和句尾标记\n",
        "        tokens = [\"[CLS]\"] + sentence + [\"[SEP]\"]\n",
        "\n",
        "        # 得到句子词语id，type和attention中的mask\n",
        "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        types = [0] * (len(ids))\n",
        "        masks = [1] * len(ids)\n",
        "        # 与pad_size比较，进行切断或填补\n",
        "        if len(ids) < pad_size:\n",
        "            #不是词语的部分，type为1，mask为0，id为0\n",
        "            types = types + [1] * (pad_size - len(ids))\n",
        "            masks = masks + [0] * (pad_size - len(ids))\n",
        "            ids = ids + [0] * (pad_size - len(ids))\n",
        "        else:\n",
        "            types = types[:pad_size]\n",
        "            masks = masks[:pad_size]\n",
        "            ids = ids[:pad_size]\n",
        "        #列表中分别存放所有训练数据的id，type，mask和label\n",
        "        word_ids.append(ids)\n",
        "        word_types.append(types)\n",
        "        word_masks.append(masks)\n",
        "        assert len(ids) == len(masks) == len(types) == pad_size\n",
        "        labels.append([int(item_type)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o10fPWL04-r9",
        "outputId": "4b8f750e-6740-42ee-cd39-bceae709a2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10000it [01:15, 132.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted(len_dict.items(), reverse=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL6qc8t3qUAk",
        "outputId": "d37d90c4-ced7-4ec5-b989-e0cae9715cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(13, 1), (15, 4), (16, 1), (18, 1), (20, 1), (23, 1), (24, 2), (25, 2), (26, 1), (28, 1), (29, 11), (30, 2), (32, 3), (33, 2), (35, 2), (36, 1), (40, 1), (41, 1), (43, 1), (45, 1), (46, 2), (47, 1), (49, 1), (50, 1), (51, 1), (52, 1), (54, 2), (55, 1), (59, 1), (60, 2), (61, 1), (62, 2), (64, 3), (65, 1), (66, 2), (67, 3), (68, 2), (69, 1), (70, 2), (71, 1), (72, 1), (73, 5), (74, 1), (75, 2), (76, 2), (77, 2), (78, 3), (79, 2), (80, 3), (81, 2), (82, 5), (83, 7), (84, 3), (85, 3), (86, 4), (87, 4), (88, 5), (89, 4), (90, 3), (91, 6), (92, 1), (93, 4), (94, 5), (95, 4), (96, 4), (97, 3), (98, 2), (99, 5), (100, 8), (101, 9), (102, 7), (103, 3), (104, 4), (105, 4), (106, 7), (107, 8), (108, 4), (109, 4), (110, 1), (111, 6), (112, 4), (113, 8), (114, 4), (115, 3), (116, 5), (117, 5), (118, 2), (119, 7), (120, 5), (121, 8), (122, 8), (123, 9), (124, 7), (125, 5), (126, 7), (127, 5), (128, 5), (129, 3), (130, 2), (131, 2), (132, 6), (133, 4), (134, 2), (135, 3), (136, 6), (137, 4), (138, 6), (139, 5), (140, 2), (141, 10), (142, 10), (143, 11), (144, 9), (145, 7), (146, 5), (147, 7), (148, 10), (149, 6), (150, 3), (151, 8), (152, 7), (153, 11), (154, 6), (155, 7), (156, 6), (157, 7), (158, 6), (159, 4), (160, 4), (161, 6), (162, 10), (163, 6), (164, 9), (165, 6), (166, 9), (167, 5), (168, 11), (169, 6), (170, 6), (171, 4), (172, 7), (173, 9), (174, 7), (175, 10), (176, 10), (177, 4), (178, 6), (179, 8), (180, 6), (181, 1), (182, 6), (183, 5), (184, 3), (185, 7), (187, 12), (188, 5), (189, 8), (190, 11), (191, 9), (192, 7), (193, 4), (194, 5), (195, 5), (196, 2), (197, 9), (198, 6), (199, 5), (200, 3), (201, 2), (202, 8), (203, 5), (204, 4), (205, 5), (206, 7), (207, 6), (208, 14), (209, 1), (210, 7), (211, 5), (212, 7), (213, 5), (214, 7), (215, 9), (216, 5), (217, 4), (218, 4), (219, 3), (220, 11), (221, 4), (222, 5), (223, 7), (224, 3), (225, 8), (226, 4), (227, 6), (228, 8), (229, 2), (230, 7), (231, 6), (232, 3), (233, 10), (234, 6), (235, 7), (236, 11), (237, 7), (238, 5), (239, 5), (240, 5), (241, 7), (242, 10), (243, 6), (244, 6), (245, 7), (246, 6), (247, 10), (248, 10), (249, 4), (250, 13), (251, 7), (252, 9), (253, 5), (254, 6), (255, 7), (256, 10), (257, 7), (258, 10), (259, 6), (260, 8), (261, 1), (262, 9), (263, 4), (264, 6), (265, 3), (266, 5), (267, 9), (268, 6), (269, 8), (270, 5), (271, 12), (272, 8), (273, 7), (274, 8), (275, 11), (276, 5), (277, 9), (278, 5), (279, 12), (280, 6), (281, 10), (282, 8), (283, 5), (284, 6), (285, 7), (286, 3), (287, 9), (288, 7), (289, 12), (290, 14), (291, 6), (292, 7), (293, 9), (294, 11), (295, 4), (296, 9), (297, 10), (298, 13), (299, 6), (300, 7), (301, 4), (302, 5), (303, 9), (304, 9), (305, 9), (306, 6), (307, 11), (308, 12), (309, 7), (310, 9), (311, 5), (312, 9), (313, 4), (314, 8), (315, 12), (316, 3), (317, 4), (318, 3), (319, 8), (320, 16), (321, 11), (322, 9), (323, 6), (324, 9), (325, 9), (326, 9), (327, 11), (328, 9), (329, 7), (330, 6), (331, 6), (332, 8), (333, 12), (334, 3), (335, 6), (336, 13), (337, 6), (338, 5), (339, 10), (340, 3), (341, 8), (342, 5), (343, 3), (344, 6), (345, 12), (346, 5), (347, 10), (348, 12), (349, 7), (350, 7), (351, 5), (352, 5), (353, 5), (354, 13), (355, 6), (356, 8), (357, 8), (358, 8), (359, 10), (360, 5), (361, 6), (362, 14), (363, 8), (364, 6), (365, 6), (366, 4), (367, 11), (368, 3), (369, 7), (370, 12), (371, 2), (372, 7), (373, 5), (374, 10), (375, 8), (376, 5), (377, 6), (378, 12), (379, 13), (380, 7), (381, 5), (382, 7), (383, 11), (384, 5), (385, 7), (386, 8), (387, 12), (388, 6), (389, 2), (390, 8), (391, 9), (392, 9), (393, 9), (394, 7), (395, 13), (396, 7), (397, 9), (398, 2), (399, 6), (400, 12), (401, 7), (402, 6), (403, 10), (404, 7), (405, 6), (406, 10), (407, 8), (408, 7), (409, 9), (410, 9), (411, 10), (412, 8), (413, 15), (414, 5), (415, 6), (416, 9), (417, 8), (418, 12), (419, 16), (420, 12), (421, 5), (422, 9), (423, 9), (424, 11), (425, 8), (426, 10), (427, 5), (428, 7), (429, 6), (430, 6), (431, 9), (432, 9), (433, 9), (434, 7), (435, 11), (436, 9), (437, 8), (438, 10), (439, 12), (440, 12), (441, 9), (442, 4), (443, 12), (444, 10), (445, 6), (446, 12), (447, 3), (448, 3), (449, 11), (450, 9), (451, 6), (452, 4), (453, 7), (454, 8), (455, 11), (456, 11), (457, 6), (458, 3), (459, 5), (460, 9), (461, 10), (462, 5), (463, 7), (464, 15), (465, 6), (466, 9), (467, 9), (468, 7), (469, 7), (470, 7), (471, 5), (472, 6), (473, 11), (474, 8), (475, 9), (476, 6), (477, 7), (478, 12), (479, 7), (480, 10), (481, 7), (482, 11), (483, 9), (484, 6), (485, 8), (486, 11), (487, 10), (488, 11), (489, 8), (490, 9), (491, 7), (492, 7), (493, 9), (494, 9), (495, 12), (496, 8), (497, 11), (498, 9), (499, 8), (500, 12), (501, 12), (502, 7), (503, 9), (504, 8), (505, 8), (506, 8), (507, 4), (508, 8), (509, 17), (510, 8), (511, 9), (512, 11), (513, 9), (514, 12), (515, 7), (516, 7), (517, 5), (518, 9), (519, 5), (520, 8), (521, 10), (522, 5), (523, 6), (524, 5), (525, 7), (526, 7), (527, 8), (528, 10), (529, 12), (530, 18), (531, 7), (532, 7), (533, 9), (534, 11), (535, 11), (536, 3), (537, 8), (538, 7), (539, 5), (540, 6), (541, 11), (542, 8), (543, 5), (544, 3), (545, 8), (546, 6), (547, 6), (548, 10), (549, 10), (550, 9), (551, 7), (552, 4), (553, 6), (554, 6), (555, 8), (556, 11), (557, 7), (558, 12), (559, 7), (560, 10), (561, 10), (562, 7), (563, 11), (564, 8), (565, 9), (566, 13), (567, 9), (568, 3), (569, 11), (570, 13), (571, 10), (572, 11), (573, 7), (574, 11), (575, 10), (576, 12), (577, 9), (578, 8), (579, 13), (580, 6), (581, 5), (582, 11), (583, 10), (584, 16), (585, 14), (586, 6), (587, 6), (588, 13), (589, 7), (590, 8), (591, 10), (592, 12), (593, 4), (594, 10), (595, 5), (596, 13), (597, 8), (598, 12), (599, 8), (600, 7), (601, 9), (602, 8), (603, 6), (604, 13), (605, 14), (606, 10), (607, 9), (608, 17), (609, 12), (610, 8), (611, 5), (612, 5), (613, 3), (614, 15), (615, 9), (616, 6), (617, 7), (618, 9), (619, 7), (620, 7), (621, 5), (622, 6), (623, 10), (624, 6), (625, 8), (626, 5), (627, 7), (628, 6), (629, 9), (630, 7), (631, 7), (632, 9), (633, 13), (634, 7), (635, 8), (636, 6), (637, 15), (638, 4), (639, 4), (640, 7), (641, 6), (642, 9), (643, 5), (644, 9), (645, 5), (646, 6), (647, 6), (648, 13), (649, 9), (650, 4), (651, 8), (652, 7), (653, 6), (654, 8), (655, 9), (656, 8), (657, 11), (658, 8), (659, 8), (660, 8), (661, 5), (662, 9), (663, 7), (664, 12), (665, 10), (666, 13), (667, 15), (668, 13), (669, 8), (670, 11), (671, 8), (672, 7), (673, 15), (674, 4), (675, 9), (676, 5), (677, 7), (678, 10), (679, 7), (680, 6), (681, 14), (682, 8), (683, 4), (684, 2), (685, 12), (686, 5), (687, 11), (688, 10), (689, 4), (690, 6), (691, 13), (692, 10), (693, 5), (694, 11), (695, 6), (696, 12), (697, 11), (698, 10), (699, 7), (700, 13), (701, 10), (702, 6), (703, 7), (704, 9), (705, 8), (706, 6), (707, 7), (708, 10), (709, 7), (710, 3), (711, 1), (712, 10), (713, 4), (714, 6), (715, 11), (716, 6), (717, 8), (718, 1), (719, 7), (720, 12), (721, 13), (722, 5), (723, 7), (724, 11), (725, 6), (726, 5), (727, 12), (728, 8), (729, 11), (730, 13), (731, 5), (732, 6), (733, 7), (734, 4), (735, 7), (736, 9), (737, 12), (738, 6), (739, 9), (740, 10), (741, 6), (742, 9), (743, 8), (744, 14), (745, 8), (746, 5), (747, 10), (748, 5), (749, 7), (750, 13), (751, 8), (752, 5), (753, 7), (754, 4), (755, 10), (756, 4), (757, 16), (758, 7), (759, 11), (760, 6), (761, 4), (762, 7), (763, 7), (764, 9), (765, 9), (766, 6), (767, 8), (768, 6), (769, 9), (770, 5), (771, 15), (772, 9), (773, 7), (774, 7), (775, 6), (776, 4), (777, 6), (778, 9), (779, 9), (780, 7), (781, 6), (782, 8), (783, 5), (784, 4), (785, 5), (786, 12), (787, 4), (788, 8), (789, 6), (790, 2), (791, 7), (792, 7), (793, 4), (794, 10), (795, 14), (796, 5), (797, 6), (798, 2), (799, 8), (800, 6), (801, 5), (802, 9), (803, 6), (804, 11), (805, 7), (806, 6), (807, 8), (808, 5), (809, 8), (810, 4), (811, 11), (812, 9), (813, 8), (814, 4), (815, 9), (816, 9), (817, 3), (818, 6), (819, 7), (820, 7), (821, 7), (822, 4), (823, 8), (824, 7), (825, 4), (826, 5), (827, 6), (828, 3), (829, 8), (830, 4), (831, 10), (832, 6), (833, 9), (834, 5), (835, 3), (836, 6), (837, 7), (838, 12), (839, 4), (840, 9), (841, 10), (842, 9), (843, 8), (844, 9), (845, 3), (846, 9), (847, 2), (848, 6), (849, 11), (850, 12), (851, 3), (852, 5), (853, 5), (854, 6), (855, 6), (856, 7), (857, 9), (858, 7), (859, 5), (860, 5), (861, 8), (862, 7), (863, 11), (864, 5), (865, 13), (866, 2), (867, 7), (868, 5), (869, 2), (870, 10), (871, 3), (872, 7), (873, 9), (874, 12), (875, 4), (876, 10), (877, 11), (878, 6), (879, 4), (880, 10), (881, 9), (882, 11), (883, 6), (884, 4), (885, 8), (886, 5), (887, 9), (888, 5), (889, 4), (890, 2), (891, 5), (892, 7), (893, 3), (894, 5), (895, 5), (896, 8), (897, 8), (898, 7), (899, 5), (900, 7), (901, 6), (902, 3), (903, 5), (904, 13), (905, 7), (906, 4), (907, 6), (908, 5), (909, 6), (910, 8), (911, 5), (912, 6), (913, 5), (914, 10), (915, 8), (916, 7), (917, 7), (918, 6), (919, 5), (920, 3), (921, 3), (922, 4), (923, 5), (924, 6), (925, 7), (926, 4), (927, 7), (928, 5), (929, 3), (930, 5), (931, 2), (932, 6), (933, 5), (934, 6), (935, 8), (936, 4), (937, 4), (938, 3), (939, 5), (940, 9), (941, 6), (942, 2), (943, 10), (944, 5), (945, 9), (946, 7), (947, 5), (948, 3), (949, 4), (950, 3), (951, 7), (952, 6), (953, 7), (954, 6), (955, 8), (956, 9), (957, 8), (958, 8), (959, 7), (960, 6), (961, 5), (962, 3), (963, 8), (964, 1), (965, 4), (966, 5), (967, 7), (968, 9), (969, 5), (970, 6), (971, 10), (972, 9), (973, 7), (974, 5), (975, 3), (976, 2), (977, 7), (978, 2), (979, 7), (980, 3), (981, 4), (982, 3), (983, 2), (984, 7), (985, 7), (986, 3), (987, 4), (988, 8), (989, 5), (990, 9), (991, 5), (992, 4), (993, 5), (994, 7), (995, 2), (996, 6), (997, 5), (998, 2), (999, 2), (1000, 5), (1001, 6), (1002, 3), (1003, 6), (1004, 5), (1005, 5), (1006, 4), (1007, 7), (1008, 7), (1009, 3), (1010, 6), (1011, 6), (1012, 1), (1013, 8), (1014, 1), (1015, 6), (1016, 6), (1017, 4), (1018, 3), (1019, 2), (1020, 5), (1021, 3), (1022, 4), (1023, 4), (1024, 5), (1025, 5), (1026, 6), (1027, 3), (1028, 1), (1029, 7), (1030, 5), (1031, 6), (1032, 4), (1033, 2), (1034, 1), (1035, 5), (1036, 5), (1037, 5), (1038, 8), (1039, 7), (1040, 6), (1041, 5), (1042, 6), (1043, 4), (1044, 6), (1045, 6), (1046, 8), (1047, 1), (1048, 11), (1049, 5), (1050, 2), (1051, 8), (1052, 7), (1053, 3), (1054, 6), (1055, 7), (1056, 3), (1057, 6), (1058, 2), (1059, 2), (1060, 3), (1061, 5), (1062, 6), (1064, 6), (1065, 4), (1066, 4), (1067, 12), (1068, 3), (1069, 8), (1070, 3), (1071, 4), (1072, 2), (1073, 4), (1074, 5), (1075, 8), (1076, 9), (1077, 4), (1078, 6), (1079, 6), (1080, 4), (1081, 8), (1082, 4), (1083, 2), (1084, 5), (1085, 1), (1086, 8), (1087, 6), (1088, 1), (1089, 2), (1090, 2), (1091, 4), (1092, 7), (1093, 3), (1094, 3), (1095, 6), (1096, 8), (1097, 5), (1098, 4), (1099, 2), (1100, 3), (1101, 4), (1102, 4), (1103, 5), (1104, 2), (1105, 2), (1106, 3), (1107, 8), (1108, 3), (1109, 7), (1110, 7), (1111, 4), (1112, 3), (1113, 5), (1114, 2), (1115, 7), (1116, 4), (1117, 3), (1118, 4), (1119, 5), (1120, 8), (1121, 4), (1122, 5), (1123, 2), (1124, 4), (1125, 5), (1126, 6), (1127, 2), (1128, 7), (1129, 2), (1130, 4), (1131, 4), (1132, 3), (1133, 4), (1134, 5), (1135, 4), (1136, 1), (1137, 1), (1138, 4), (1139, 5), (1140, 3), (1141, 1), (1142, 3), (1144, 1), (1145, 4), (1146, 8), (1147, 1), (1148, 3), (1149, 5), (1150, 4), (1151, 5), (1152, 2), (1153, 5), (1154, 4), (1155, 3), (1156, 4), (1157, 4), (1158, 5), (1159, 6), (1160, 2), (1161, 4), (1162, 5), (1163, 3), (1164, 3), (1166, 5), (1167, 3), (1168, 9), (1169, 1), (1170, 3), (1171, 4), (1172, 4), (1173, 2), (1174, 6), (1175, 3), (1176, 7), (1177, 3), (1178, 6), (1179, 6), (1180, 3), (1181, 2), (1182, 1), (1183, 3), (1184, 6), (1185, 4), (1186, 3), (1187, 6), (1188, 4), (1189, 4), (1190, 5), (1191, 2), (1192, 2), (1193, 2), (1195, 5), (1196, 1), (1197, 4), (1198, 2), (1199, 4), (1200, 6), (1201, 5), (1202, 2), (1203, 2), (1204, 6), (1205, 6), (1206, 1), (1207, 3), (1208, 3), (1209, 1), (1210, 3), (1211, 4), (1212, 2), (1213, 8), (1214, 1), (1215, 2), (1216, 1), (1217, 2), (1218, 4), (1219, 2), (1220, 3), (1221, 6), (1222, 2), (1223, 1), (1224, 3), (1225, 4), (1226, 4), (1227, 1), (1228, 3), (1229, 1), (1230, 1), (1231, 3), (1232, 3), (1233, 3), (1234, 6), (1236, 4), (1237, 1), (1238, 1), (1239, 5), (1241, 5), (1242, 1), (1243, 3), (1244, 3), (1245, 7), (1246, 4), (1247, 4), (1248, 2), (1249, 2), (1250, 1), (1251, 5), (1252, 3), (1253, 2), (1254, 3), (1255, 3), (1256, 3), (1257, 5), (1258, 5), (1259, 1), (1260, 6), (1261, 3), (1263, 3), (1264, 2), (1265, 3), (1266, 2), (1267, 2), (1268, 4), (1269, 2), (1270, 1), (1271, 3), (1272, 1), (1273, 4), (1274, 1), (1275, 4), (1276, 1), (1277, 1), (1278, 3), (1279, 2), (1280, 2), (1281, 3), (1282, 5), (1283, 4), (1284, 5), (1285, 3), (1286, 2), (1287, 2), (1288, 1), (1289, 4), (1290, 4), (1291, 1), (1292, 1), (1293, 2), (1294, 1), (1295, 4), (1296, 5), (1297, 4), (1298, 3), (1299, 5), (1300, 1), (1301, 1), (1302, 2), (1303, 3), (1304, 2), (1305, 4), (1306, 1), (1308, 1), (1310, 3), (1311, 1), (1312, 2), (1313, 3), (1314, 6), (1315, 3), (1316, 4), (1317, 1), (1318, 2), (1319, 3), (1320, 5), (1321, 3), (1322, 1), (1323, 6), (1324, 3), (1326, 3), (1327, 1), (1328, 4), (1329, 4), (1330, 4), (1331, 2), (1332, 8), (1333, 6), (1334, 4), (1335, 3), (1336, 2), (1337, 1), (1338, 3), (1339, 2), (1340, 2), (1341, 2), (1342, 3), (1344, 1), (1345, 2), (1346, 4), (1349, 2), (1350, 7), (1351, 2), (1352, 4), (1353, 5), (1355, 2), (1356, 1), (1357, 1), (1358, 1), (1359, 3), (1360, 1), (1361, 3), (1362, 4), (1363, 1), (1364, 1), (1365, 2), (1366, 4), (1367, 2), (1368, 2), (1369, 4), (1370, 2), (1371, 2), (1372, 1), (1373, 3), (1374, 1), (1375, 1), (1376, 1), (1377, 3), (1378, 3), (1379, 1), (1380, 2), (1381, 2), (1382, 2), (1383, 5), (1384, 2), (1385, 6), (1386, 3), (1387, 4), (1389, 1), (1390, 3), (1391, 1), (1393, 2), (1394, 1), (1395, 3), (1396, 4), (1397, 2), (1398, 1), (1400, 3), (1401, 2), (1402, 3), (1403, 4), (1404, 2), (1406, 2), (1407, 2), (1408, 1), (1409, 3), (1410, 4), (1412, 3), (1413, 3), (1414, 4), (1415, 1), (1416, 5), (1417, 2), (1418, 3), (1419, 5), (1420, 1), (1421, 3), (1422, 2), (1423, 4), (1424, 1), (1425, 4), (1426, 2), (1427, 5), (1429, 3), (1430, 3), (1431, 1), (1432, 6), (1433, 5), (1434, 2), (1436, 3), (1437, 4), (1438, 1), (1439, 6), (1440, 1), (1441, 1), (1442, 2), (1443, 3), (1445, 1), (1446, 3), (1447, 4), (1448, 5), (1449, 2), (1450, 3), (1452, 2), (1453, 3), (1454, 1), (1455, 1), (1456, 4), (1457, 1), (1458, 1), (1459, 2), (1460, 1), (1461, 2), (1462, 2), (1463, 3), (1464, 2), (1465, 5), (1466, 4), (1467, 2), (1468, 4), (1469, 1), (1471, 3), (1472, 1), (1473, 6), (1474, 3), (1475, 1), (1476, 3), (1477, 3), (1479, 1), (1480, 1), (1481, 1), (1482, 1), (1484, 3), (1485, 5), (1486, 4), (1487, 4), (1488, 1), (1489, 1), (1490, 1), (1491, 5), (1493, 1), (1494, 5), (1495, 3), (1496, 3), (1497, 1), (1499, 3), (1500, 6), (1502, 3), (1503, 1), (1504, 2), (1505, 2), (1506, 2), (1508, 1), (1509, 3), (1510, 1), (1511, 5), (1512, 3), (1513, 2), (1514, 1), (1515, 3), (1516, 1), (1517, 3), (1518, 2), (1519, 2), (1520, 3), (1521, 2), (1522, 2), (1523, 1), (1524, 1), (1525, 4), (1526, 2), (1528, 1), (1529, 1), (1530, 4), (1531, 1), (1532, 1), (1533, 2), (1535, 2), (1536, 1), (1537, 2), (1538, 3), (1539, 2), (1540, 3), (1541, 2), (1542, 1), (1543, 3), (1544, 1), (1546, 1), (1547, 3), (1548, 3), (1549, 1), (1550, 1), (1551, 5), (1552, 4), (1553, 3), (1555, 1), (1556, 1), (1557, 1), (1558, 1), (1559, 1), (1560, 3), (1561, 2), (1563, 1), (1564, 2), (1565, 1), (1566, 3), (1567, 2), (1570, 1), (1571, 4), (1574, 2), (1575, 3), (1576, 4), (1578, 4), (1580, 1), (1582, 1), (1586, 3), (1587, 1), (1588, 4), (1590, 3), (1591, 3), (1592, 2), (1595, 2), (1597, 2), (1600, 4), (1602, 2), (1603, 1), (1605, 1), (1607, 2), (1608, 4), (1609, 1), (1611, 1), (1612, 2), (1613, 2), (1614, 3), (1616, 4), (1617, 3), (1618, 1), (1619, 2), (1620, 1), (1622, 1), (1623, 1), (1625, 1), (1627, 3), (1628, 1), (1630, 2), (1632, 3), (1635, 2), (1638, 6), (1639, 2), (1640, 1), (1641, 3), (1642, 1), (1644, 1), (1645, 4), (1648, 1), (1649, 3), (1650, 3), (1651, 4), (1653, 2), (1654, 1), (1657, 1), (1658, 3), (1659, 2), (1660, 1), (1663, 1), (1664, 1), (1665, 1), (1666, 5), (1667, 1), (1668, 1), (1670, 2), (1671, 1), (1672, 2), (1674, 3), (1676, 2), (1677, 1), (1678, 1), (1679, 3), (1680, 1), (1683, 2), (1684, 2), (1685, 4), (1686, 5), (1687, 1), (1688, 1), (1689, 3), (1690, 1), (1691, 1), (1692, 2), (1693, 3), (1694, 2), (1695, 3), (1696, 1), (1697, 2), (1699, 3), (1700, 2), (1701, 3), (1704, 1), (1705, 3), (1706, 2), (1707, 1), (1709, 1), (1710, 1), (1711, 2), (1713, 1), (1714, 2), (1717, 1), (1718, 2), (1719, 1), (1720, 1), (1723, 1), (1724, 6), (1727, 1), (1728, 2), (1729, 4), (1730, 1), (1732, 2), (1733, 1), (1735, 3), (1736, 1), (1737, 2), (1738, 2), (1740, 1), (1741, 4), (1742, 2), (1744, 1), (1745, 3), (1746, 2), (1747, 2), (1748, 1), (1749, 1), (1751, 3), (1752, 3), (1754, 2), (1755, 3), (1756, 1), (1757, 4), (1758, 2), (1759, 1), (1760, 2), (1761, 1), (1762, 1), (1763, 1), (1764, 1), (1765, 2), (1766, 2), (1767, 3), (1768, 1), (1770, 2), (1772, 1), (1773, 5), (1775, 1), (1776, 4), (1777, 2), (1778, 1), (1779, 3), (1781, 2), (1783, 4), (1784, 1), (1787, 1), (1788, 1), (1789, 1), (1791, 1), (1792, 3), (1793, 3), (1794, 2), (1795, 3), (1796, 2), (1797, 1), (1798, 1), (1799, 3), (1800, 1), (1801, 2), (1802, 1), (1803, 2), (1804, 2), (1806, 1), (1807, 1), (1809, 3), (1810, 1), (1811, 1), (1812, 2), (1813, 1), (1814, 3), (1815, 1), (1816, 3), (1817, 1), (1819, 2), (1821, 1), (1822, 2), (1823, 4), (1826, 1), (1827, 2), (1828, 1), (1829, 1), (1830, 3), (1831, 1), (1833, 1), (1834, 3), (1835, 1), (1836, 4), (1837, 4), (1838, 4), (1839, 2), (1840, 1), (1841, 2), (1843, 1), (1844, 2), (1845, 3), (1847, 1), (1849, 1), (1851, 2), (1853, 3), (1854, 1), (1856, 1), (1857, 2), (1860, 2), (1861, 1), (1862, 2), (1863, 1), (1865, 2), (1866, 1), (1868, 2), (1870, 1), (1871, 2), (1872, 1), (1873, 1), (1875, 1), (1876, 1), (1877, 1), (1880, 1), (1881, 2), (1882, 1), (1885, 1), (1889, 2), (1890, 1), (1894, 1), (1897, 2), (1899, 3), (1900, 1), (1901, 1), (1903, 2), (1905, 3), (1907, 2), (1908, 1), (1909, 1), (1910, 2), (1911, 1), (1912, 4), (1915, 1), (1916, 1), (1921, 1), (1922, 1), (1924, 2), (1925, 1), (1926, 1), (1928, 4), (1931, 1), (1933, 3), (1934, 3), (1935, 2), (1939, 2), (1940, 1), (1944, 1), (1945, 1), (1946, 2), (1947, 1), (1948, 1), (1950, 1), (1951, 2), (1952, 3), (1953, 2), (1954, 3), (1957, 1), (1960, 1), (1961, 1), (1962, 1), (1963, 1), (1964, 2), (1966, 2), (1967, 1), (1968, 1), (1969, 1), (1970, 1), (1971, 1), (1973, 4), (1974, 2), (1975, 2), (1976, 3), (1978, 4), (1979, 2), (1980, 2), (1982, 1), (1983, 2), (1984, 1), (1987, 1), (1988, 1), (1989, 2), (1990, 1), (1991, 1), (1992, 2), (1993, 1), (1995, 1), (1997, 1), (1998, 1), (1999, 2), (2000, 3), (2001, 1), (2003, 2), (2004, 1), (2011, 3), (2015, 2), (2019, 1), (2020, 1), (2021, 1), (2022, 1), (2025, 4), (2026, 1), (2029, 1), (2030, 2), (2031, 1), (2032, 1), (2033, 1), (2034, 1), (2035, 1), (2036, 2), (2037, 2), (2038, 2), (2040, 1), (2042, 1), (2044, 1), (2045, 3), (2046, 1), (2049, 1), (2052, 1), (2053, 1), (2056, 1), (2057, 3), (2058, 1), (2059, 2), (2060, 1), (2061, 1), (2062, 2), (2064, 1), (2066, 1), (2070, 3), (2073, 5), (2074, 1), (2076, 1), (2077, 1), (2078, 2), (2081, 1), (2084, 1), (2085, 1), (2086, 3), (2087, 2), (2091, 1), (2092, 1), (2093, 1), (2094, 2), (2095, 3), (2096, 1), (2097, 1), (2098, 2), (2099, 2), (2101, 1), (2103, 1), (2104, 1), (2105, 2), (2107, 1), (2109, 1), (2110, 1), (2112, 2), (2113, 1), (2114, 1), (2115, 1), (2120, 1), (2121, 2), (2123, 2), (2124, 2), (2127, 1), (2128, 1), (2136, 3), (2142, 1), (2143, 2), (2145, 1), (2150, 1), (2151, 1), (2153, 2), (2154, 1), (2155, 1), (2158, 1), (2159, 1), (2162, 1), (2163, 2), (2168, 3), (2169, 1), (2170, 1), (2173, 2), (2177, 3), (2178, 1), (2180, 2), (2182, 2), (2188, 1), (2190, 2), (2191, 1), (2194, 1), (2196, 2), (2197, 4), (2198, 1), (2199, 2), (2201, 1), (2202, 1), (2204, 1), (2205, 1), (2207, 2), (2208, 1), (2212, 1), (2213, 1), (2214, 1), (2215, 1), (2219, 3), (2220, 1), (2222, 1), (2225, 2), (2228, 1), (2229, 1), (2231, 2), (2232, 2), (2236, 3), (2239, 1), (2242, 1), (2243, 2), (2244, 1), (2245, 2), (2246, 2), (2247, 3), (2248, 1), (2249, 1), (2253, 1), (2254, 1), (2255, 1), (2259, 2), (2260, 2), (2262, 1), (2267, 2), (2269, 1), (2270, 1), (2271, 2), (2272, 2), (2276, 5), (2278, 1), (2279, 1), (2284, 1), (2286, 1), (2291, 2), (2292, 2), (2299, 1), (2302, 1), (2303, 1), (2304, 1), (2305, 1), (2307, 3), (2309, 2), (2310, 2), (2313, 1), (2314, 1), (2318, 2), (2320, 4), (2323, 1), (2324, 1), (2326, 1), (2329, 2), (2330, 3), (2331, 3), (2333, 3), (2334, 1), (2335, 1), (2338, 1), (2339, 1), (2340, 1), (2341, 2), (2342, 4), (2347, 1), (2348, 2), (2351, 2), (2354, 1), (2355, 1), (2357, 1), (2358, 1), (2361, 2), (2362, 1), (2363, 1), (2364, 1), (2365, 1), (2366, 2), (2369, 1), (2370, 2), (2372, 1), (2377, 2), (2380, 1), (2381, 1), (2382, 1), (2383, 1), (2384, 1), (2386, 1), (2387, 1), (2388, 1), (2391, 1), (2392, 1), (2394, 1), (2395, 1), (2396, 2), (2399, 1), (2400, 1), (2401, 1), (2402, 3), (2408, 1), (2412, 2), (2414, 1), (2415, 2), (2416, 1), (2419, 1), (2421, 2), (2423, 5), (2424, 1), (2425, 1), (2432, 1), (2434, 1), (2437, 1), (2438, 1), (2440, 2), (2443, 1), (2448, 1), (2449, 2), (2452, 1), (2454, 1), (2455, 2), (2456, 1), (2458, 2), (2459, 1), (2460, 2), (2461, 1), (2462, 1), (2466, 2), (2467, 3), (2468, 1), (2469, 1), (2472, 1), (2475, 2), (2476, 1), (2477, 2), (2479, 1), (2481, 3), (2482, 2), (2484, 1), (2485, 3), (2491, 1), (2494, 1), (2495, 1), (2496, 1), (2497, 1), (2499, 2), (2500, 1), (2501, 2), (2502, 1), (2504, 3), (2505, 1), (2506, 1), (2507, 2), (2510, 1), (2513, 1), (2516, 1), (2518, 1), (2519, 1), (2523, 1), (2524, 1), (2525, 1), (2526, 2), (2527, 3), (2529, 1), (2531, 1), (2534, 1), (2549, 1), (2551, 1), (2552, 1), (2553, 1), (2556, 1), (2557, 1), (2558, 1), (2561, 1), (2565, 1), (2570, 1), (2574, 2), (2576, 1), (2578, 1), (2579, 3), (2581, 1), (2582, 1), (2583, 2), (2585, 1), (2586, 1), (2588, 2), (2591, 1), (2592, 2), (2597, 1), (2598, 1), (2603, 1), (2604, 1), (2605, 1), (2614, 2), (2615, 2), (2618, 1), (2622, 1), (2624, 1), (2626, 1), (2627, 1), (2628, 1), (2630, 1), (2631, 1), (2632, 1), (2633, 1), (2645, 1), (2646, 1), (2647, 1), (2649, 1), (2650, 1), (2651, 1), (2658, 1), (2659, 1), (2660, 1), (2662, 1), (2664, 1), (2665, 2), (2666, 1), (2667, 1), (2669, 1), (2672, 1), (2674, 1), (2676, 1), (2680, 1), (2688, 1), (2693, 1), (2697, 1), (2703, 1), (2707, 1), (2708, 1), (2711, 1), (2712, 1), (2713, 1), (2716, 1), (2717, 1), (2722, 1), (2728, 1), (2729, 1), (2734, 1), (2737, 1), (2741, 1), (2742, 1), (2746, 1), (2747, 1), (2755, 2), (2762, 1), (2763, 1), (2766, 1), (2767, 3), (2768, 1), (2775, 1), (2788, 1), (2789, 1), (2790, 2), (2792, 1), (2794, 1), (2796, 1), (2803, 1), (2804, 1), (2806, 1), (2809, 1), (2812, 1), (2814, 1), (2816, 1), (2819, 1), (2826, 1), (2833, 2), (2834, 2), (2837, 1), (2838, 1), (2840, 2), (2841, 1), (2844, 1), (2846, 1), (2847, 1), (2848, 1), (2851, 1), (2855, 1), (2857, 1), (2858, 1), (2864, 1), (2865, 1), (2867, 1), (2869, 1), (2872, 2), (2875, 1), (2882, 1), (2884, 1), (2885, 1), (2889, 1), (2891, 1), (2892, 1), (2895, 1), (2897, 1), (2901, 1), (2902, 3), (2903, 1), (2904, 1), (2905, 2), (2907, 1), (2908, 1), (2909, 1), (2910, 2), (2912, 1), (2916, 1), (2918, 1), (2919, 1), (2920, 1), (2925, 1), (2926, 1), (2928, 1), (2929, 1), (2934, 1), (2940, 1), (2946, 2), (2951, 1), (2957, 1), (2958, 1), (2963, 1), (2966, 1), (2967, 1), (2968, 1), (2969, 1), (2970, 1), (2978, 2), (2980, 1), (2982, 1), (2984, 1), (2986, 1), (2987, 1), (2990, 1), (3006, 1), (3011, 1), (3013, 1), (3022, 1), (3023, 1), (3028, 1), (3033, 1), (3036, 1), (3045, 1), (3047, 1), (3054, 1), (3055, 1), (3056, 1), (3057, 1), (3058, 1), (3062, 1), (3066, 1), (3067, 1), (3073, 1), (3076, 1), (3078, 1), (3082, 1), (3084, 1), (3086, 2), (3089, 1), (3095, 1), (3096, 2), (3108, 1), (3110, 1), (3112, 2), (3114, 1), (3120, 1), (3125, 1), (3126, 1), (3127, 1), (3130, 1), (3136, 1), (3137, 1), (3144, 1), (3151, 3), (3154, 1), (3162, 1), (3163, 3), (3164, 1), (3166, 1), (3167, 1), (3169, 1), (3170, 1), (3173, 1), (3176, 1), (3179, 1), (3181, 1), (3183, 2), (3197, 1), (3200, 1), (3201, 1), (3203, 1), (3212, 1), (3215, 2), (3219, 1), (3222, 1), (3223, 1), (3236, 1), (3239, 1), (3241, 1), (3244, 1), (3245, 1), (3248, 1), (3250, 1), (3263, 1), (3264, 1), (3268, 1), (3279, 1), (3281, 1), (3289, 1), (3295, 1), (3297, 1), (3310, 1), (3316, 1), (3317, 1), (3318, 2), (3325, 1), (3327, 1), (3329, 1), (3332, 1), (3334, 1), (3337, 1), (3341, 1), (3343, 1), (3346, 1), (3348, 1), (3354, 1), (3361, 1), (3368, 1), (3370, 1), (3371, 1), (3374, 1), (3375, 1), (3377, 1), (3383, 2), (3392, 1), (3402, 2), (3408, 1), (3411, 1), (3415, 1), (3416, 1), (3427, 1), (3430, 1), (3435, 1), (3442, 1), (3444, 1), (3445, 1), (3447, 1), (3451, 2), (3461, 1), (3465, 1), (3471, 1), (3487, 1), (3514, 1), (3519, 1), (3523, 2), (3525, 1), (3534, 2), (3559, 1), (3567, 1), (3569, 1), (3570, 1), (3600, 1), (3607, 1), (3637, 1), (3662, 1), (3670, 1), (3683, 1), (3687, 1), (3688, 1), (3692, 1), (3704, 1), (3705, 1), (3708, 1), (3712, 1), (3717, 1), (3722, 1), (3728, 1), (3729, 1), (3732, 2), (3734, 1), (3737, 1), (3738, 1), (3742, 1), (3758, 2), (3761, 1), (3767, 1), (3781, 1), (3784, 1), (3804, 1), (3805, 1), (3811, 1), (3820, 1), (3824, 1), (3825, 1), (3836, 1), (3855, 1), (3856, 1), (3857, 1), (3863, 1), (3867, 1), (3877, 1), (3883, 1), (3892, 1), (3893, 1), (3897, 1), (3914, 1), (3918, 1), (3921, 1), (3923, 1), (3925, 1), (3943, 1), (3947, 1), (3968, 1), (3970, 1), (3973, 1), (3975, 1), (3982, 1), (3984, 1), (4005, 1), (4036, 1), (4039, 1), (4048, 1), (4065, 1), (4085, 1), (4109, 1), (4124, 1), (4127, 1), (4134, 1), (4139, 1), (4164, 1), (4165, 1), (4175, 1), (4176, 1), (4181, 1), (4183, 1), (4186, 1), (4203, 1), (4213, 1), (4222, 1), (4234, 1), (4236, 1), (4239, 1), (4267, 1), (4280, 1), (4293, 1), (4315, 1), (4328, 1), (4329, 1), (4332, 1), (4351, 1), (4412, 1), (4417, 1), (4419, 1), (4456, 1), (4462, 1), (4471, 1), (4494, 1), (4502, 1), (4504, 1), (4507, 1), (4530, 1), (4538, 1), (4555, 1), (4595, 1), (4602, 1), (4628, 1), (4641, 1), (4652, 1), (4684, 1), (4685, 1), (4705, 1), (4718, 1), (4728, 1), (4729, 1), (4753, 1), (4754, 1), (4760, 1), (4814, 1), (4815, 1), (4816, 1), (4829, 2), (4839, 1), (4860, 1), (4878, 1), (4938, 1), (4939, 1), (4944, 1), (4983, 1), (4984, 1), (4988, 1), (5084, 1), (5086, 1), (5153, 1), (5162, 1), (5203, 1), (5273, 1), (5276, 1), (5300, 1), (5346, 1), (5348, 1), (5493, 1), (5528, 1), (5550, 1), (5651, 1), (5681, 1), (5776, 1), (5785, 1), (5792, 1), (5835, 1), (5839, 1), (5865, 1), (5975, 2), (6099, 1), (6153, 1), (6160, 1), (6209, 1), (6243, 1), (6270, 1), (6348, 1), (6382, 1), (6395, 1), (6405, 1), (6562, 1), (6566, 1), (6668, 1), (6727, 1), (6729, 1), (6924, 1), (6942, 1), (7045, 1), (7315, 1), (7325, 1), (7730, 1), (7993, 1), (8095, 1), (8165, 1), (8560, 1), (8589, 1), (8767, 1), (8954, 1), (9056, 1), (9169, 1), (9208, 1), (9308, 1), (10787, 1), (11732, 1), (12171, 1), (12389, 1), (12641, 1), (13308, 1), (13389, 1), (14720, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成随机数列表，打乱索引，随机划分数据集\n",
        "random_order = list(range(len(word_ids)))\n",
        "np.random.seed(2022)\n",
        "np.random.shuffle(random_order)\n",
        "print(random_order[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob6v75tt5GVZ",
        "outputId": "a09bd588-86f2-4596-cec4-360c142c8b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6487, 8785, 7390, 7078, 1230, 3684, 5263, 3533, 104, 8080]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 按照4:1比例划分训练集和测试集\n",
        "# 将数据集格式转换为np array格式\n",
        "input_ids_train = np.array([word_ids[i] for i in random_order[:int(len(word_ids) * 0.8)]])\n",
        "input_types_train = np.array([word_types[i] for i in random_order[:int(len(word_ids) * 0.8)]])\n",
        "input_masks_train = np.array([word_masks[i] for i in random_order[:int(len(word_ids) * 0.8)]])\n",
        "y_train = np.array([labels[i] for i in random_order[:int(len(word_ids) * 0.8)]])\n",
        "print(input_ids_train.shape, input_types_train.shape, input_masks_train.shape, y_train.shape)\n",
        "\n",
        "input_ids_test = np.array([word_ids[i] for i in random_order[int(len(word_ids) * 0.8):]])\n",
        "input_types_test = np.array([word_types[i] for i in random_order[int(len(word_ids) * 0.8):]])\n",
        "input_masks_test = np.array([word_masks[i] for i in random_order[int(len(word_ids) * 0.8):]])\n",
        "y_test = np.array([labels[i] for i in random_order[int(len(word_ids) * 0.8):]])\n",
        "print(input_ids_test.shape, input_types_test.shape, input_masks_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVnLp5nK5IgX",
        "outputId": "9a01e1fd-ab48-4348-c51e-257926dcdc66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8000, 50) (8000, 50) (8000, 50) (8000, 1)\n",
            "(2000, 50) (2000, 50) (2000, 50) (2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成训练集与测试集的dataloader\n",
        "train_data = TensorDataset(torch.LongTensor(input_ids_train), torch.LongTensor(input_types_train),\n",
        "                           torch.LongTensor(input_masks_train), torch.LongTensor(y_train))\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_data = TensorDataset(torch.LongTensor(input_ids_test),torch.LongTensor(input_types_test),\n",
        "                          torch.LongTensor(input_masks_test), torch.LongTensor(y_test))\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "FiROgQxZ5I-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.定义模型"
      ],
      "metadata": {
        "id": "_-3CLazPFvvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        # 加载预训练的bert模型权重\n",
        "        self.bert = BertModel.from_pretrained(bert_path) \n",
        "        # 对于每个参数均求梯度\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True \n",
        "        # 线性层求10个类别的概率    \n",
        "        self.fc = nn.Linear(768, 10)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        sentence = x[0]  \n",
        "        types = x[1]\n",
        "        mask = x[2]  \n",
        "        _, pooled = self.bert(sentence, token_type_ids=types, \n",
        "                              attention_mask=mask, output_all_encoded_layers=False)\n",
        "        out = self.fc(pooled)\n",
        "        return out"
      ],
      "metadata": {
        "id": "dl8AueQU5Pnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 若有可用设备则使用cuda进行计算，否则cpu计算\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "model = Model().to(DEVICE)\n",
        "print(DEVICE)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RjZ0HDg5RKC",
        "outputId": "488e2fac-e756-4a6a-d40d-acac0816f079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Model(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): BertLayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=768, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.定义优化器"
      ],
      "metadata": {
        "id": "YuvrvhfQGDA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成参数名列表和不进行权重衰减的列表\n",
        "param_optimizer = list(model.named_parameters())  \n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "# 使用BertAdam优化器\n",
        "optimizer = BertAdam(optimizer_grouped_parameters, lr=optimizer_lr, warmup=optimizer_warmup, t_total=len(train_loader) * NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "KyMamqra5TAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.训练与测试"
      ],
      "metadata": {
        "id": "CVUqRJTbGFGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练模型\n",
        "def train(model, device, train_loader, optimizer, epoch):  \n",
        "    model.train()\n",
        "    # 计算训练时间\n",
        "    start_time = time.time()\n",
        "    # 加载训练数据\n",
        "    for batch_idx, (sentence, types, masks, item_type) in enumerate(train_loader):\n",
        "        # 将训练数据转移到计算的设备上\n",
        "        sentence, types, masks, item_type = sentence.to(device), types.to(device), masks.to(device), item_type.to(device)\n",
        "        # 生成预测结果\n",
        "        y_pred = model([sentence, types, masks])  \n",
        "        # 梯度清零，计算损失并进行误差反向传播\n",
        "        model.zero_grad()  \n",
        "        loss = F.cross_entropy(y_pred, item_type.squeeze()) \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # 打印训练信息\n",
        "        if (batch_idx + 1) % 100 == 0:  \n",
        "            current_time = time.time()\n",
        "            print('Train Epoch: {} [{}/{} ({:.2f}%)]tLoss: {:.6f}  Time:{:.2f} s'.format(epoch,\n",
        "                                           (batch_idx + 1) * len(sentence), len(train_loader.dataset),\n",
        "                                           100. * batch_idx / len(train_loader), loss.item(),\n",
        "                                           (current_time-start_time))) \n",
        "\n",
        "# 测试模型\n",
        "def test(model, device, test_loader):  \n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    acc = 0\n",
        "    # 加载测试数据\n",
        "    for batch_idx, (sentence, types, masks, item_type) in enumerate(test_loader):\n",
        "        sentence, types, masks, item_type = sentence.to(device), types.to(device), masks.to(device), item_type.to(device)\n",
        "        # 测试过程中不计算梯度\n",
        "        with torch.no_grad():\n",
        "            y_pred = model([sentence, types, masks])\n",
        "        # 得到预测的句子类别\n",
        "        pred = y_pred.max(-1, keepdim=True)[1] \n",
        "        # 计算测试损失、准确率\n",
        "        test_loss += F.cross_entropy(y_pred, item_type.squeeze())\n",
        "        acc += pred.eq(item_type.view_as(pred)).sum().item() \n",
        "    test_loss /= len(test_loader)\n",
        "    # 打印测试信息\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(test_loss, acc, \n",
        "                                        len(test_loader.dataset), 100. * acc / len(test_loader.dataset)))\n",
        "    return acc / len(test_loader.dataset)"
      ],
      "metadata": {
        "id": "X8NTw3Ln5azz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0.982\n",
        "# 训练模型，保存正确率最高的模型\n",
        "for epoch in range(1, NUM_EPOCHS + 1):  # 3个epoch\n",
        "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
        "    acc = test(model, DEVICE, test_loader)\n",
        "    if best_acc < acc:\n",
        "        best_acc = acc\n",
        "        # 保存模型，记录模型正确率\n",
        "        torch.save(model.state_dict(), ckpt_path+'roberta_model_' + str(round(acc, 3)) + '.pth') \n",
        "    # 打印训练信息\n",
        "    print(\"Train Epoch: {} acc is: {:.4f}, best acc is {:.4f}n\".format(epoch, acc, best_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "ouDyAugl5bU8",
        "outputId": "bb39f277-5802-4af7-fdb2-94b39f3c3115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [1600/8000 (19.80%)]tLoss: 0.053531  Time:10.57 s\n",
            "Train Epoch: 1 [3200/8000 (39.80%)]tLoss: 0.266350  Time:11.74 s\n",
            "Train Epoch: 1 [4800/8000 (59.80%)]tLoss: 0.015487  Time:11.73 s\n",
            "Train Epoch: 1 [6400/8000 (79.80%)]tLoss: 0.186178  Time:12.74 s\n",
            "Train Epoch: 1 [8000/8000 (99.80%)]tLoss: 0.247608  Time:12.00 s\n",
            "Test set: Average loss: 0.0753, Accuracy: 1954/2000 (97.70%)\n",
            "Train Epoch: 1 acc is: 0.9770, best acc is 0.9820n\n",
            "Train Epoch: 2 [1600/8000 (19.80%)]tLoss: 0.039600  Time:11.22 s\n",
            "Train Epoch: 2 [3200/8000 (39.80%)]tLoss: 0.021227  Time:10.94 s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-99eebf045323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 训练模型，保存正确率最高的模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 3个epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-3f2190529151>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# 打印训练信息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 预测句子类别\n",
        "def predict_type(sentence, path):\n",
        "    # 加载最优的模型\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    \n",
        "    sentence = tokenizer.tokenize(sentence)\n",
        "    tokens = [\"[CLS]\"] + sentence + [\"[SEP]\"]\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    types = [0] * (len(ids))\n",
        "    masks = [1] * len(ids)\n",
        "    # 与pad_size比较，进行切断或填补\n",
        "    if len(ids) < pad_size:\n",
        "        types = types + [1] * (pad_size - len(ids))  \n",
        "        masks = masks + [0] * (pad_size - len(ids))\n",
        "        ids = ids + [0] * (pad_size - len(ids))\n",
        "    else:\n",
        "        types = types[:pad_size]\n",
        "        masks = masks[:pad_size]\n",
        "        ids = ids[:pad_size]\n",
        "\n",
        "    ids, types, masks = torch.LongTensor(np.array(ids)), torch.LongTensor(np.array(types)), torch.LongTensor(\n",
        "        np.array(masks))\n",
        "\n",
        "    y_pred = model([ids.reshape(1, -1), types.reshape(1, -1), masks.reshape(1, -1)])\n",
        "\n",
        "    return sentence_types[torch.argmax(y_pred)]"
      ],
      "metadata": {
        "id": "HFGBUnF35est"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "types=predict_type(\"人民币汇率降低\", \"/content/drive/MyDrive/data/ckpt/roberta_model.pth\")\n",
        "print(types)\n",
        "types=predict_type(\"梅西获得卡塔尔世界杯金球奖\", \"/content/drive/MyDrive/data/ckpt/roberta_model.pth\")\n",
        "print(types)"
      ],
      "metadata": {
        "id": "cI1V2eZz5f7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dacea7b4-a161-4567-c333-3448aa4cd4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "时政\n",
            "体育\n"
          ]
        }
      ]
    }
  ]
}