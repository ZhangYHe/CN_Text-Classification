# -*- coding: utf-8 -*-
"""BIT2022_NLU_h2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_35Uju3Da5Erqti7rtTX8fJ0N4HpiivQ

"""
"""##1.导入需要的库"""

from google.colab import drive
drive.mount('/content/drive')

#!pip install pytorch_pretrained_bert

# coding: UTF-8
import torch
import time 
import torch.nn as nn
import torch.nn.functional as F 
from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig, BertAdam
import pandas as pd 
import numpy as np 
from tqdm import tqdm 
from torch.utils.data import Dataset, TensorDataset,RandomSampler,DataLoader,SequentialSampler

"""##2.数据处理"""

# word_ids存放词语的id
# word_types中0、1区分不同句子
# word_masks为attention中的掩码，0表示padding
word_ids = []
word_types = []
word_masks = []
labels = []
pad_size = 50
# 句子类别共10种
sentence_types = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']
len_dict = dict()

# data_path为数据集路径，bert_path为预训练模型权重路径,ckpt_path为模型路径
data_path = "./data/cnews/"
bert_path = "./chinese_roberta_wwm_ext_pytorch/"
ckpt_path = "./data/ckpt/"
# 初始化分词器，使用预训练的vocab
tokenizer = BertTokenizer(vocab_file=bert_path + "vocab.txt")

BATCH_SIZE = 16
NUM_EPOCHS = 3
optimizer_lr = 2e-5
optimizer_warmup = 0.05

# 读取训练数据
with open(data_path + "cnews.test.txt", encoding='utf-8') as f:
    for i, item in tqdm(enumerate(f)):
        # 统计句子长度，存储在字典中
        if (len(item)-4) not in len_dict:
          len_dict[len(item)-4] = 1
        else:
          len_dict[len(item)-4] += 1
        # 句子类别为字符串第一个词语
        item_type = sentence_types.index(item[0:2])
        sentence = tokenizer.tokenize(item[4:-1])
        # 添加bert中句首和句尾标记
        tokens = ["[CLS]"] + sentence + ["[SEP]"]

        # 得到句子词语id，type和attention中的mask
        ids = tokenizer.convert_tokens_to_ids(tokens)
        types = [0] * (len(ids))
        masks = [1] * len(ids)
        # 与pad_size比较，进行切断或填补
        if len(ids) < pad_size:
            #不是词语的部分，type为1，mask为0，id为0
            types = types + [1] * (pad_size - len(ids))
            masks = masks + [0] * (pad_size - len(ids))
            ids = ids + [0] * (pad_size - len(ids))
        else:
            types = types[:pad_size]
            masks = masks[:pad_size]
            ids = ids[:pad_size]
        #列表中分别存放所有训练数据的id，type，mask和label
        word_ids.append(ids)
        word_types.append(types)
        word_masks.append(masks)
        assert len(ids) == len(masks) == len(types) == pad_size
        labels.append([int(item_type)])

print(sorted(len_dict.items(), reverse=False))

# 生成随机数列表，打乱索引，随机划分数据集
random_order = list(range(len(word_ids)))
np.random.seed(2022)
np.random.shuffle(random_order)
print(random_order[:10])

# 按照4:1比例划分训练集和测试集
# 将数据集格式转换为np array格式
input_ids_train = np.array([word_ids[i] for i in random_order[:int(len(word_ids) * 0.8)]])
input_types_train = np.array([word_types[i] for i in random_order[:int(len(word_ids) * 0.8)]])
input_masks_train = np.array([word_masks[i] for i in random_order[:int(len(word_ids) * 0.8)]])
y_train = np.array([labels[i] for i in random_order[:int(len(word_ids) * 0.8)]])
print(input_ids_train.shape, input_types_train.shape, input_masks_train.shape, y_train.shape)

input_ids_test = np.array([word_ids[i] for i in random_order[int(len(word_ids) * 0.8):]])
input_types_test = np.array([word_types[i] for i in random_order[int(len(word_ids) * 0.8):]])
input_masks_test = np.array([word_masks[i] for i in random_order[int(len(word_ids) * 0.8):]])
y_test = np.array([labels[i] for i in random_order[int(len(word_ids) * 0.8):]])
print(input_ids_test.shape, input_types_test.shape, input_masks_test.shape, y_test.shape)

# 生成训练集与测试集的dataloader
train_data = TensorDataset(torch.LongTensor(input_ids_train), torch.LongTensor(input_types_train),
                           torch.LongTensor(input_masks_train), torch.LongTensor(y_train))
train_sampler = RandomSampler(train_data)
train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)

test_data = TensorDataset(torch.LongTensor(input_ids_test),torch.LongTensor(input_types_test),
                          torch.LongTensor(input_masks_test), torch.LongTensor(y_test))
test_sampler = SequentialSampler(test_data)
test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)

"""## 3.定义模型"""

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        # 加载预训练的bert模型权重
        self.bert = BertModel.from_pretrained(bert_path) 
        # 对于每个参数均求梯度
        for param in self.bert.parameters():
            param.requires_grad = True 
        # 线性层求10个类别的概率    
        self.fc = nn.Linear(768, 10)  

    def forward(self, x):
        sentence = x[0]  
        types = x[1]
        mask = x[2]  
        _, pooled = self.bert(sentence, token_type_ids=types, 
                              attention_mask=mask, output_all_encoded_layers=False)
        out = self.fc(pooled)
        return out

# 若有可用设备则使用cuda进行计算，否则cpu计算
if torch.cuda.is_available():
    DEVICE = torch.device("cuda")
else:
    DEVICE = torch.device("cpu")
model = Model().to(DEVICE)
print(DEVICE)
print(model)

"""## 4.定义优化器"""

# 生成参数名列表和不进行权重衰减的列表
param_optimizer = list(model.named_parameters())  
no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]

# 使用BertAdam优化器
optimizer = BertAdam(optimizer_grouped_parameters, lr=optimizer_lr, warmup=optimizer_warmup, t_total=len(train_loader) * NUM_EPOCHS)

"""## 5.训练与测试"""

# 训练模型
def train(model, device, train_loader, optimizer, epoch):  
    model.train()
    # 计算训练时间
    start_time = time.time()
    # 加载训练数据
    for batch_idx, (sentence, types, masks, item_type) in enumerate(train_loader):
        # 将训练数据转移到计算的设备上
        sentence, types, masks, item_type = sentence.to(device), types.to(device), masks.to(device), item_type.to(device)
        # 生成预测结果
        y_pred = model([sentence, types, masks])  
        # 梯度清零，计算损失并进行误差反向传播
        model.zero_grad()  
        loss = F.cross_entropy(y_pred, item_type.squeeze()) 
        loss.backward()
        optimizer.step()
        # 打印训练信息
        if (batch_idx + 1) % 100 == 0:  
            current_time = time.time()
            print('Train Epoch: {} [{}/{} ({:.2f}%)]tLoss: {:.6f}  Time:{:.2f} s'.format(epoch,
                                           (batch_idx + 1) * len(sentence), len(train_loader.dataset),
                                           100. * batch_idx / len(train_loader), loss.item(),
                                           (current_time-start_time))) 

# 测试模型
def test(model, device, test_loader):  
    model.eval()
    test_loss = 0.0
    acc = 0
    # 加载测试数据
    for batch_idx, (sentence, types, masks, item_type) in enumerate(test_loader):
        sentence, types, masks, item_type = sentence.to(device), types.to(device), masks.to(device), item_type.to(device)
        # 测试过程中不计算梯度
        with torch.no_grad():
            y_pred = model([sentence, types, masks])
        # 得到预测的句子类别
        pred = y_pred.max(-1, keepdim=True)[1] 
        # 计算测试损失、准确率
        test_loss += F.cross_entropy(y_pred, item_type.squeeze())
        acc += pred.eq(item_type.view_as(pred)).sum().item() 
    test_loss /= len(test_loader)
    # 打印测试信息
    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(test_loss, acc, 
                                        len(test_loader.dataset), 100. * acc / len(test_loader.dataset)))
    return acc / len(test_loader.dataset)

best_acc = 0.982
# 训练模型，保存正确率最高的模型
for epoch in range(1, NUM_EPOCHS + 1):  # 3个epoch
    train(model, DEVICE, train_loader, optimizer, epoch)
    acc = test(model, DEVICE, test_loader)
    if best_acc < acc:
        best_acc = acc
        # 保存模型，记录模型正确率
        torch.save(model.state_dict(), ckpt_path+'roberta_model_' + str(round(acc, 3)) + '.pth') 
    # 打印训练信息
    print("Train Epoch: {} acc is: {:.4f}, best acc is {:.4f}n".format(epoch, acc, best_acc))

# 预测句子类别
def predict_type(sentence, path):
    # 加载最优的模型
    model.load_state_dict(torch.load(path))
    
    sentence = tokenizer.tokenize(sentence)
    tokens = ["[CLS]"] + sentence + ["[SEP]"]
    ids = tokenizer.convert_tokens_to_ids(tokens)
    types = [0] * (len(ids))
    masks = [1] * len(ids)
    # 与pad_size比较，进行切断或填补
    if len(ids) < pad_size:
        types = types + [1] * (pad_size - len(ids))  
        masks = masks + [0] * (pad_size - len(ids))
        ids = ids + [0] * (pad_size - len(ids))
    else:
        types = types[:pad_size]
        masks = masks[:pad_size]
        ids = ids[:pad_size]

    ids, types, masks = torch.LongTensor(np.array(ids)), torch.LongTensor(np.array(types)), torch.LongTensor(
        np.array(masks))

    y_pred = model([ids.reshape(1, -1), types.reshape(1, -1), masks.reshape(1, -1)])

    return sentence_types[torch.argmax(y_pred)]

types=predict_type("人民币汇率降低", "./data/ckpt/roberta_model.pth")
print(types)
types=predict_type("梅西获得卡塔尔世界杯金球奖", "./data/ckpt/roberta_model.pth")
print(types)